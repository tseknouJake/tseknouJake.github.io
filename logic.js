function setClickableTooltip(target, content){
    $( target ).tooltip({
      show: null, // show immediately
      position: { my: "center top", at: "center bottom+10"},
      content: content, //from params
      hide: { effect: "" }, //fadeOut
      tooltipClass: "bib_tooltip",
      close: function(event, ui){
        ui.tooltip.hover(
          function () {
            $(this).stop(true).fadeTo(400, 1);
          },
          function () {
            $(this).fadeOut("400", function(){
              $(this).remove();
            })
          }
        );
      }
    });
  }
  
  setClickableTooltip('#bib_hughes2021', ``);
  
  setClickableTooltip('#bib_yao2021', ``);
  
  setClickableTooltip('#bib_orel2020', `@inproceedings{orel2020lifespan, <br>
       &emsp; title={Lifespan Age Transformation Synthesis}, <br>
       &emsp; author={Or-El, Roy and Sengupta, Soumyadip and Fried, Ohad and Shechtman, Eli and Kemelmacher-Shlizerman, Ira}, <br>
       &emsp; booktitle={Proceedings of the European Conference on Computer Vision (ECCV)}, <br>
       &emsp; year={2020} <br>
  }`);
  
  setClickableTooltip('#bib_agarwal2020', `@inproceedings{Agarwal_2020_CVPR_Workshops, <br>
      &emsp; author = {Agarwal, Shruti and Farid, Hany and Fried, Ohad and Agrawala, Maneesh}, <br>
      &emsp; title = {Detecting Deep-Fake Videos From Phoneme-Viseme Mismatches}, <br>
      &emsp; booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops}, <br>
      &emsp; month = {June}, <br>
      &emsp; year = {2020} <br>
  }`);
  
  setClickableTooltip('#bib_e2020', `@inproceedings{10.1145/3313831.3376635, <br>
      &emsp; author = {E, Jane L. and Fried, Ohad and Lu, Jingwan and Zhang, Jianming and Mech, Radom\\'{\\i}r and Echevarria, Jose and Hanrahan, Pat and Landay, James A.}, <br>
      &emsp; title = {Adaptive Photographic Composition Guidance}, <br>
      &emsp; year = {2020}, <br>
      &emsp; isbn = {9781450367080}, <br>
      &emsp; publisher = {Association for Computing Machinery}, <br>
      &emsp; address = {New York, NY, USA}, <br>
      &emsp; url = {https://doi.org/10.1145/3313831.3376635}, <br>
      &emsp; doi = {10.1145/3313831.3376635}, <br>
      &emsp; booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, <br>
      &emsp; pages = {1–13}, <br>
      &emsp; numpages = {13}, <br>
      &emsp; keywords = {dynamic symmetry, composition, camera interfaces, photography}, <br>
      &emsp; location = {Honolulu, HI, USA}, <br>
      &emsp; series = {CHI ’20} <br>
  }`);
  
  setClickableTooltip('#bib_tewari2020', "\
  @article{10.1111:cgf.14022, <br>\
      &emsp; journal = {Computer Graphics Forum}, <br>\
      &emsp; title = {{State of the Art on Neural Rendering}}, <br>\
      &emsp; author = {Tewari, Ayush and Fried, Ohad and Thies, Justus and Sitzmann, Vincent and Lombardi, Stephen and Sunkavalli, Kalyan and Martin-Brualla, Ricardo and Simon, Tomas and Saragih, Jason and Nießner, Matthias and Pandey, Rohit and Fanello, Sean and Wetzstein, Gordon and Zhu, Jun-Yan and Theobalt, Christian and Agrawala, Maneesh and Shechtman, Eli and Goldman, Dan B. and Zollhöfer, Michael}, <br>\
      &emsp; year = {2020}, <br>\
      &emsp; publisher = {The Eurographics Association and John Wiley & Sons Ltd.}, <br>\
      &emsp; ISSN = {1467-8659}, <br>\
      &emsp; DOI = {10.1111/cgf.14022} <br>\
  } <br>\
  ");
  
  setClickableTooltip('#bib_fried2020a', "\
  @inproceedings{10.1145/3326601, <br>\
      &emsp; author = {Fried, Ohad and Jacobs, Jennifer and Finkelstein, Adam and Agrawala, Maneesh}, <br>\
      &emsp; title = {Editing Self-Image}, <br>\
      &emsp; year = {2020}, <br>\
      &emsp; issue_date = {February 2020}, <br>\
      &emsp; publisher = {Association for Computing Machinery}, <br>\
      &emsp; address = {New York, NY, USA}, <br>\
      &emsp; volume = {63}, <br>\
      &emsp; number = {3}, <br>\
      &emsp; issn = {0001-0782}, <br>\
      &emsp; url = {https://doi.org/10.1145/3326601}, <br>\
      &emsp; doi = {10.1145/3326601}, <br>\
      &emsp; journal = {Commun. ACM}, <br>\
      &emsp; month = feb, <br>\
      &emsp; pages = {70–79}, <br>\
      &emsp; numpages = {10} <br>\
  }  <br>\
  ");
  
  setClickableTooltip('#bib_e2019', "\
  @inproceedings{10.1145/3332165.3347893, <br>\
      &emsp; author = {E, Jane L. and Fried, Ohad and Agrawala, Maneesh}, <br>\
      &emsp; title = {Optimizing Portrait Lighting at Capture-Time Using a 360 Camera as a Light Probe}, <br>\
      &emsp; year = {2019}, <br>\
      &emsp; isbn = {9781450368162}, <br>\
      &emsp; publisher = {Association for Computing Machinery}, <br>\
      &emsp; address = {New York, NY, USA}, <br>\
      &emsp; url = {https://doi.org/10.1145/3332165.3347893}, <br>\
      &emsp; doi = {10.1145/3332165.3347893}, <br>\
      &emsp; booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology}, <br>\
      &emsp; pages = {221–232}, <br>\
      &emsp; numpages = {12}, <br>\
      &emsp; keywords = {environment map, 360 camera, portrait lighting}, <br>\
      &emsp; location = {New Orleans, LA, USA}, <br>\
      &emsp; series = {UIST ’19} <br>\
  }  <br>\
  ");
  
  setClickableTooltip('#bib_fried2019c', "\
  @inproceedings{.20191220, <br>\
      &emsp; booktitle = {Eurographics Symposium on Rendering - DL-only and Industry Track}, <br>\
      &emsp; editor = {Boubekeur, Tamy and Sen, Pradeep}, <br>\
      &emsp; title = {{Puppet Dubbing}}, <br>\
      &emsp; author = {Fried, Ohad and Agrawala, Maneesh}, <br>\
      &emsp; year = {2019}, <br>\
      &emsp; publisher = {The Eurographics Association}, <br>\
      &emsp; ISSN = {1727-3463}, <br>\
      &emsp; ISBN = {978-3-03868-095-6}, <br>\
      &emsp; DOI = {10.2312/sr.20191220} <br>\
  }  <br>\
  ");
  
  setClickableTooltip('#bib_fried2019b', "\
  @article{Fried:2019:TET:3306346.3323028, <br>\
      &emsp; author = {Fried, Ohad and Tewari, Ayush and Zollh\\\"{o}fer, Michael and Finkelstein, Adam and Shechtman, Eli and Goldman, Dan B and Genova, Kyle and Jin, Zeyu and Theobalt, Christian and Agrawala, Maneesh}, <br>\
      &emsp; title = {Text-based Editing of Talking-head Video}, <br>\
      &emsp; journal = {ACM Trans. Graph.}, <br>\
      &emsp; issue_date = {July 2019}, <br>\
      &emsp; volume = {38}, <br>\
      &emsp; number = {4}, <br>\
      &emsp; month = jul, <br>\
      &emsp; year = {2019}, <br>\
      &emsp; issn = {0730-0301}, <br>\
      &emsp; pages = {68:1--68:14}, <br>\
      &emsp; articleno = {68}, <br>\
      &emsp; numpages = {14}, <br>\
      &emsp; url = {http://doi.acm.org/10.1145/3306346.3323028}, <br>\
      &emsp; doi = {10.1145/3306346.3323028}, <br>\
      &emsp; acmid = {3323028}, <br>\
      &emsp; publisher = {ACM}, <br>\
      &emsp; address = {New York, NY, USA}, <br>\
      &emsp; keywords = {dubbing, face parameterization, face tracking, neural rendering, talking heads, text-based video editing, visemes} <br>\
  }  <br>\
  ");
  
  setClickableTooltip('#bib_fried2019a', "\
  @article{arXiv:1902.04285, <br>\
      &emsp; author = {Fried O and Agrawala M}, <br>\
      &emsp; title = {Puppet Dubbing}, <br>\
      &emsp; journal = {arXiv}, <br>\
      &emsp; volume = {}, <br>\
      &emsp; number = {}, <br>\
      &emsp; pages = {}, <br>\
      &emsp; year = {2019}, <br>\
      &emsp; url = {https://arxiv.org/abs/1902.04285}, <br>\
  }  <br>\
  ");
  
  setClickableTooltip('#bib_danon2019', "\
  @article{Danon2019, <br>\
      &emsp; author = {Danon, Dov and Averbuch-Elor, Hadar and Fried, Ohad and Cohen-Or, Daniel}, <br>\
      &emsp; title = {Unsupervised natural image patch learning}, <br>\
      &emsp; journal = {Computational Visual Media}, <br>\
      &emsp; year = {2019}, <br>\
      &emsp; month = {Aug}, <br>\
      &emsp; day = {22}, <br>\
      &emsp; issn = {2096-0662}, <br>\
      &emsp; doi = {10.1007/s41095-019-0147-y}, <br>\
      &emsp; url = {https://doi.org/10.1007/s41095-019-0147-y}, <br>\
  }  <br>\
  ");
  
  setClickableTooltip('#bib_danon2018', "\
  @article{arXiv:1807.03130, <br>\
      &emsp; author = {Danon D and Averbuch-Elor H and Fried O and Cohen-Or D}, <br>\
      &emsp; title = {Unsupervised Natural Image Patch Learning}, <br>\
      &emsp; journal = {arXiv}, <br>\
      &emsp; volume = {}, <br>\
      &emsp; number = {}, <br>\
      &emsp; pages = {}, <br>\
      &emsp; year = {2018}, <br>\
      &emsp; url = {https://arxiv.org/abs/1807.03130}, <br>\
  }  <br>\
  ");
  
  setClickableTooltip('#bib_fried2018', "\
  @article{doi:10.1001/jamafacial.2018.0652, <br>\
      &emsp; author = {Fried O and Paskhover B}, <br>\
      &emsp; title = {Perceived facial distortions in selfies are explained by viewing habits—reply}, <br>\
      &emsp; journal = {JAMA Facial Plastic Surgery}, <br>\
      &emsp; volume = {}, <br>\
      &emsp; number = {}, <br>\
      &emsp; pages = {}, <br>\
      &emsp; year = {2018}, <br>\
      &emsp; doi = {10.1001/jamafacial.2018.0652}, <br>\
      &emsp; url = {http://dx.doi.org/10.1001/jamafacial.2018.0652}, <br>\
  }  <br>\
  ");
  
  setClickableTooltip('#bib_ward2018', "\
  @article{doi:10.1001/jamafacial.2018.0009, <br>\
      &emsp; author = {Ward B and Ward M and Fried O and Paskhover B}, <br>\
      &emsp; title = {Nasal distortion in short-distance photographs: The selfie effect}, <br>\
      &emsp; journal = {JAMA Facial Plastic Surgery}, <br>\
      &emsp; volume = {20}, <br>\
      &emsp; number = {4}, <br>\
      &emsp; pages = {333-335}, <br>\
      &emsp; year = {2018}, <br>\
      &emsp; doi = {10.1001/jamafacial.2018.0009}, <br>\
      &emsp; url = {http://dx.doi.org/10.1001/jamafacial.2018.0009}, <br>\
  }  <br>\
  ");
  
  setClickableTooltip('#bib_fried2017b', "\
  @article{CGF:CGF13284, <br>\
      &emsp; author = {Fried, O. and Avidan, S. and Cohen-Or, D.}, <br>\
      &emsp; title = {Patch2Vec: Globally Consistent Image Patch Representation}, <br>\
      &emsp; journal = {Computer Graphics Forum}, <br>\
      &emsp; volume = {36}, <br>\
      &emsp; number = {7}, <br>\
      &emsp; issn = {1467-8659}, <br>\
      &emsp; url = {http://dx.doi.org/10.1111/cgf.13284}, <br>\
      &emsp; doi = {10.1111/cgf.13284}, <br>\
      &emsp; pages = {183--194}, <br>\
      &emsp; keywords = {Categories and Subject Descriptors (according to ACM CCS), I.4.10 [Image Processing and Computer Vision]: Image Representation—Multidimensional}, <br>\
      &emsp; year = {2017}, <br>\
  }  <br>\
  ");
  
  setClickableTooltip('#bib_fried2017a', "\
  @phdthesis{fried2017, <br>\
      &emsp; author = {Fried, Ohad}, <br>\
      &emsp; year = {2017}, <br>\
      &emsp; title = {Photo Manipulation, the Easy Way}, <br>\
      &emsp; journal = {ProQuest Dissertations and Theses}, <br>\
      &emsp; pages = {101}, <br>\
      &emsp; keywords = {Applied sciences; Distractors; Editing; Faces; Manipulation; Photo; Selection; Computer Engineering; Computer science; 0464:Computer Engineering; 0984:Computer science}, <br>\
      &emsp; isbn = {9780355059984}, <br>\
      &emsp; language = {English}, <br>\
      &emsp; url = {https://search.proquest.com/docview/1928888635?accountid=14026}, <br>\
  }  <br>\
  ");
  
  setClickableTooltip('#bib_fried2016', "\
  @article{Fried:2016:PMP:2897824.2925933, <br>\
      &emsp; author = {Fried, Ohad and Shechtman, Eli and Goldman, Dan B. and Finkelstein, Adam}, <br>\
      &emsp; title = {Perspective-aware Manipulation of Portrait Photos}, <br>\
      &emsp; journal = {ACM Trans. Graph.}, <br>\
      &emsp; issue_date = {July 2016}, <br>\
      &emsp; volume = {35}, <br>\
      &emsp; number = {4}, <br>\
      &emsp; month = jul, <br>\
      &emsp; year = {2016}, <br>\
      &emsp; issn = {0730-0301}, <br>\
      &emsp; pages = {128:1--128:10}, <br>\
      &emsp; articleno = {128}, <br>\
      &emsp; numpages = {10}, <br>\
      &emsp; url = {http://doi.acm.org/10.1145/2897824.2925933}, <br>\
      &emsp; doi = {10.1145/2897824.2925933}, <br>\
      &emsp; acmid = {2925933}, <br>\
      &emsp; publisher = {ACM}, <br>\
      &emsp; address = {New York, NY, USA}, <br>\
      &emsp; keywords = {faces, image enhancement, perspective, portraits}, <br>\
  }  <br>\
  ");
  
  setClickableTooltip('#bib_chang2015', "\
  @article{Chang:2015:PPR:2809654.2766978, <br>\
      &emsp; author = {Chang, Huiwen and Fried, Ohad and Liu, Yiming and DiVerdi, Stephen and Finkelstein, Adam}, <br>\
      &emsp; title = {Palette-based Photo Recoloring}, <br>\
      &emsp; journal = {ACM Trans. Graph. (Proc. SIGGRAPH)}, <br>\
      &emsp; issue_date = {August 2015}, <br>\
      &emsp; volume = {34}, <br>\
      &emsp; number = {4}, <br>\
      &emsp; month = jul, <br>\
      &emsp; year = {2015}, <br>\
      &emsp; issn = {0730-0301}, <br>\
      &emsp; pages = {139:1--139:11}, <br>\
      &emsp; articleno = {139}, <br>\
      &emsp; numpages = {11}, <br>\
      &emsp; url = {http://doi.acm.org/10.1145/2766978}, <br>\
      &emsp; doi = {10.1145/2766978}, <br>\
      &emsp; acmid = {2766978}, <br>\
      &emsp; publisher = {ACM}, <br>\
      &emsp; address = {New York, NY, USA}, <br>\
      &emsp; keywords = {color transformation, palette, photo recoloring} <br>\
  } <br>\
  ");
  
  setClickableTooltip('#bib_fried2015b', "\
  @INPROCEEDINGS{7298779, <br>\
      &emsp; author={Fried, O. and Shechtman, E. and Goldman, D.B. and Finkelstein, A.}, <br>\
      &emsp; booktitle={Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on}, <br>\
      &emsp; title={Finding distractors in images}, <br>\
      &emsp; year={2015}, <br>\
      &emsp; pages={1703-1712}, <br>\
      &emsp; abstract={We propose a new computer vision task we call &quot;distractor prediction.&quot; Distractors are the regions of an image that draw attention away from the main subjects and reduce the overall image quality. Removing distractors-for example, using in-painting - can improve the composition of an image. In this work we created two datasets of images with user annotations to identify the characteristics of distractors. We use these datasets to train an algorithm to predict distractor maps. Finally, we use our predictor to automatically enhance images.}, <br>\
      &emsp; keywords={computer vision;image enhancement;computer vision task;distractor maps prediction;distractor prediction task;distractors characteristics;image composition;image enhancement;image quality;user annotation;Detectors;Face;Image segmentation;Mobile communication;Predictive models;Training;Visualization}, <br>\
      &emsp; doi={10.1109/CVPR.2015.7298779}, <br>\
      &emsp; month={June} <br>\
  } <br>\
  ");
  
  setClickableTooltip('#bib_fried2015a', "\
  @article {CGF:CGF12549, <br>\
      &emsp; author = {Fried, O. and DiVerdi, S. and Halber, M. and Sizikova, E. and Finkelstein, A.}, <br>\
      &emsp; title = {IsoMatch: Creating Informative Grid Layouts}, <br>\
      &emsp; journal = {Computer Graphics Forum (Proc. Eurographics)}, <br>\
      &emsp; volume = {34}, <br>\
      &emsp; number = {2}, <br>\
      &emsp; issn = {1467-8659}, <br>\
      &emsp; url = {http://dx.doi.org/10.1111/cgf.12549}, <br>\
      &emsp; doi = {10.1111/cgf.12549}, <br>\
      &emsp; pages = {155--166}, <br>\
      &emsp; keywords = {Categories and Subject Descriptors (according to ACM CCS), I.3.3 [Computer Graphics]: Picture/Image Generation-Display algorithms, I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction techniques}, <br>\
      &emsp; year = {2015}, <br>\
      &emsp; abstract = {Collections of objects such as images are often presented visually in a grid because it is a compact representation that lends itself well for search and exploration. Most grid layouts are sorted using very basic criteria, such as date or filename. In this work we present a method to arrange collections of objects respecting an arbitrary distance measure. Pairwise distances are preserved as much as possible, while still producing the specific target arrangement which may be a 2D grid, the surface of a sphere, a hierarchy, or any other shape. We show that our method can be used for infographics, collection exploration, summarization, data visualization, and even for solving problems such as where to seat family members at a wedding. We present a fast algorithm that can work on large collections and quantitatively evaluate how well distances are preserved.} <br>\
  } <br>\
  ");
  
  setClickableTooltip('#bib_fried2014', "\
  @inproceedings{ofried:2014, <br>\
      &emsp; Abstract = {The modern musician enjoys access to a staggering number of audio samples. Composition software can ship with many gigabytes of data, and there are many more to be found online. However, conventional methods for navigating these libraries are still quite rudimentary, and often involve scrolling through alphabetical lists. We present a system for sample exploration that allows audio clips to be sorted according to user taste, and arranged in any desired 2D formation such that similar samples are located near each other. Our method relies on two advances in machine learning. First, metric learning allows the user to shape the audio feature space to match their own preferences. Second, kernelized sorting finds an optimal arrangement for the samples in 2D. We demonstrate our system with two new interfaces for exploring audio samples, and evaluate the technology qualitatively and quantitatively via a pair of user studies.}, <br>\
      &emsp; Address = {London, United Kingdom}, <br>\
      &emsp; Author = {Ohad Fried and Zeyu Jin and Reid Oda and Adam Finkelstein}, <br>\
      &emsp; Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression}, <br>\
      &emsp; Editor = {Caramiaux, Baptiste and Tahiroglu, Koray and Fiebrink, Rebecca and Tanaka, Atau}, <br>\
      &emsp; Month = {June 30 -- July 03}, <br>\
      &emsp; Pages = {281--286}, <br>\
      &emsp; Publisher = {Goldsmiths, University of London}, <br>\
      &emsp; Title = {AudioQuilt: 2D Arrangements of Audio Samples using Metric Learning and Kernelized Sorting}, <br>\
      &emsp; Url = {http://www.nime.org/proceedings/2014/nime2014_315.pdf}, <br>\
      &emsp; Year = {2014}, <br>\
      &emsp; Bdsk-Url-1 = {http://www.nime.org/proceedings/2014/nime2014_315.pdf} <br>\
  } <br>\
  ");
  
  setClickableTooltip('#bib_fried2013', "\
  @inproceedings{Fried:2013, <br>\
      &emsp; Abstract = {We present a method for automatic feature extraction and cross-modal mapping using deep learning. Our system uses stacked autoencoders to learn a layered feature representation of the data. Feature vectors from two (or more) different domains are mapped to each other, effectively creating a cross-modal mapping. Our system can either run fully unsupervised, or it can use high-level labeling to fine-tune the mapping according a user's needs. We show several applications for our method, mapping sound to or from images or gestures. We evaluate system performance both in standalone inference tasks and in cross-modal mappings.}, <br>\
      &emsp; Address = {Daejeon, Republic of Korea}, <br>\
      &emsp; Author = {Ohad Fried and Rebecca Fiebrink}, <br>\
      &emsp; Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression}, <br>\
      &emsp; Date-Modified = {2013-08-16 18:22:08 +0000}, <br>\
      &emsp; Editor = {Yeo, W. and Lee, K. and A. Sigman and Ji H. and G. Wakefield}, <br>\
      &emsp; Keywords = {Deep learning, feature learning, mapping, gestural control}, <br>\
      &emsp; Month = {May 27-30}, <br>\
      &emsp; Pages = {531--534}, <br>\
      &emsp; Publisher = {Graduate School of Culture Technology, KAIST}, <br>\
      &emsp; Title = {Cross-modal Sound Mapping Using Deep Learning}, <br>\
      &emsp; Url = {http://www.nime.org/proceedings/2013/}, <br>\
      &emsp; Year = {2013}, <br>\
      &emsp; Bdsk-Url-1 = {http://www.nime.org/proceedings/2013/} <br>\
  } <br>\
  ");
  
  